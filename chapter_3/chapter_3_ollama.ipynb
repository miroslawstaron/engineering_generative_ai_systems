{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Client-server example\n",
    "\n",
    "In this script, we test the ollama framework instead of the raw HF approach. This is a WS option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Client-server using JSON API\n",
    "\n",
    "First, we generate the program that implements a specific problem using a client-server architecture. The client sends a request to the server, and the server returns the result. The communication is done using JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "url = 'http://deeperthought.cse.chalmers.se:5050/api/generate'\n",
    "data = {\n",
    "    \"model\": \"llama3.1:70b\",\n",
    "    \"prompt\": \"Write a program to calculate Fibonacci numbers in Python.\",\n",
    "    \"stream\": False\n",
    "}\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "start_time = time.time()  # Start the timer\n",
    "\n",
    "response = requests.post(url, data=json.dumps(data), headers=headers)\n",
    "\n",
    "end_time = time.time()  # End the timer\n",
    "\n",
    "json_data = json.loads(response.text)\n",
    "\n",
    "strProgram = json_data['response']\n",
    "\n",
    "print(strProgram)\n",
    "print(f\"Time taken for the request: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a local model\n",
    "\n",
    "Now, we use a local model to do the same thing. Please note that the model is much smaller because of the local hardware limitations. \n",
    "\n",
    "This code uses the Phi-3-mini model, which is a smaller version of the Phi-3 model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miroslaw/venvs/standards_n6/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68bc70ee36a04079bf5b76c03d88a5dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bff6913e4c6749978718e9b31a2f52f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miroslaw/venvs/standards_n6/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': ' Certainly! Below is a Python program that calculates Fibonacci numbers. This program includes both an iterative approach and a recursive approach for calculating Fibonacci numbers. The iterative approach is more efficient in terms of time complexity, especially for large numbers.\\n\\n### Iterative Approach\\n\\n```python\\ndef fibonacci_iterative(n):\\n    \"\"\"\\n    Calculate the nth Fibonacci number using an iterative approach.\\n    \\n    :param n: Index of the Fibonacci sequence to calculate\\n    :return: The nth Fibonacci number\\n    \"\"\"\\n    if n <= 0:\\n        return \"Input must be a positive integer\"\\n    elif n == 1:\\n        return 0\\n    elif n == 2:\\n        return 1\\n    \\n    a, b = 0, 1\\n    for _ in range(2, n):\\n        a, b = b, a + b\\n    return b\\n\\n# Example usage\\nn = 10\\nprint(f\"The {n}th Fibonacci number (iterative): {fibonacci_iterative(n)}\")\\n```\\n\\n### Recursive Approach\\n\\n```python\\ndef fibonacci_recursive(n):\\n    \"\"\"\\n    Calculate the nth Fibonacci number using a recursive approach.\\n    \\n    :param n: Index of the Fibonacci sequence to calculate\\n    :return: The nth Fibonacci number\\n    \"\"\"\\n    if n <= 0:\\n        return \"Input must be a positive integer\"\\n    elif n == 1:\\n        return 0\\n    elif n == 2:\\n        return 1\\n    \\n    return fibonacci_recursive(n-1) + fibonacci_recursive(n-2)\\n\\n# Example usage\\nn = 10\\nprint(f\"The {n}th Fibonacci number (recursive): {fibonacci_recursive(n)}\")\\n```\\n\\n### Memoization (Optimized Recursive Approach)\\n\\nThe recursive approach can be optimized using memoization to avoid recalculating Fibonacci numbers that have already been computed.'}]\n",
      "Time taken for the request: 431.10201621055603 seconds\n"
     ]
    }
   ],
   "source": [
    "# download the Phi-3-mini model from HF \n",
    "# and use the instruct pipeline \n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import time\n",
    "\n",
    "modelP = \"microsoft/Phi-3-mini-128k-instruct\"\n",
    "\n",
    "modelInstr = AutoModelForCausalLM.from_pretrained(\n",
    "    modelP, \n",
    "    device_map=\"cuda\", \n",
    "    torch_dtype=\"auto\", \n",
    "    trust_remote_code=True, \n",
    "    attn_implementation='eager',\n",
    ")\n",
    "\n",
    "tokenizerInstr = AutoTokenizer.from_pretrained(modelP)\n",
    "\n",
    "messages = [\n",
    "        {\"role\": \"user\", \"content\": \"Write a program to calculate Fibonacci numbers in Python.\"},\n",
    "]\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=modelP,\n",
    "    tokenizer=tokenizerInstr,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 500,\n",
    "    \"return_full_text\": False,\n",
    "    \"temperature\": 0.0,\n",
    "}\n",
    "\n",
    "start_time = time.time()  # Start the timer\n",
    "\n",
    "output = pipe(messages, **generation_args)\n",
    "\n",
    "end_time = time.time()  # End the timer\n",
    "\n",
    "print(output)\n",
    "print(f\"Time taken for the request: {end_time - start_time} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "standards_n6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
