{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = \"gpt2\"  # Pretrained model\n",
    "#MODEL_NAME = './decBERTa_instruction_model'  # Pretrained model\n",
    "DATASET_NAME = \"iamtarun/python_code_instructions_18k_alpaca\"\n",
    "OUTPUT_DIR = \"./gpt2_instruction_model\"\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 5e-3\n",
    "MAX_LEN = 512  # Maximum sequence length\n",
    "#MAX_LEN = 148  # Maximum sequence length\n",
    "\n",
    "# Load Dataset\n",
    "def load_data():\n",
    "    dataset = load_dataset(DATASET_NAME)\n",
    "    return dataset[\"train\"].train_test_split(test_size=0.1)\n",
    "\n",
    "def preprocess_data(examples, tokenizer):\n",
    "    # Combine 'instruction' and 'input' for each example in the batch\n",
    "    inputs = [instruction + \"\\n\" + inp for instruction, inp in zip(examples[\"instruction\"], examples[\"input\"])]\n",
    "    targets = examples[\"output\"]\n",
    "    \n",
    "    # Tokenize inputs and targets\n",
    "    model_inputs = tokenizer(inputs, max_length=MAX_LEN, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(targets, max_length=MAX_LEN, truncation=True, padding=\"max_length\")[\"input_ids\"]\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, padding_side=\"left\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    # Set the pad token if it's not already defined\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Load and preprocess data\n",
    "    datasets = load_data()\n",
    "    tokenized_datasets = datasets.map(preprocess_data, batched=True, fn_kwargs={\"tokenizer\": tokenizer})\n",
    "\n",
    "    # Set training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        save_steps=1_000,\n",
    "        save_total_limit=2,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=500,\n",
    "        fp16=torch.cuda.is_available(),  # Enable mixed precision if GPU is available\n",
    "        push_to_hub=False,\n",
    "    )\n",
    "\n",
    "    # Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Save the model\n",
    "    trainer.save_model(OUTPUT_DIR)\n",
    "    tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_code_instruction(prompt, max_length=MAX_LEN):\n",
    "    # Load the tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR, padding_side=\"left\")\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(OUTPUT_DIR, is_decoder=True)\n",
    "\n",
    "    # Tokenize the input prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=max_length, truncation=True)\n",
    "\n",
    "    # Generate the output\n",
    "    outputs = model.generate(**inputs, max_length=max_length, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    # Decode the generated output\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Example usage\n",
    "prompt = \"Write a Python function to calculate one random number.\"\n",
    "generated_instruction = generate_code_instruction(prompt)\n",
    "print(generated_instruction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "standards_n6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
