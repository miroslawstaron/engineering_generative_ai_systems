{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "M1oqh0F6W3ad"
   },
   "source": [
    "# Training a BERT model from scratch\n",
    "\n",
    "This demo illustrates the training of a BERT model from scratch. The model is trained on the tet  \n",
    "\n",
    "The notebook is based on the existing tutorial from Hugging Face [link](https://huggingface.co/blog/how-to-train).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "aleXCJqVQPro"
   },
   "source": [
    "# Training a tokenizer\n",
    "\n",
    "The first step in training the model is to train the tokenizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just checking if CUDA is available on this computer\n",
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IMnymRDLe0hi",
    "outputId": "b7780f27-ce3b-4a3f-fcf5-ba11529b1d44",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We use the standard BPE tokenizer for this workbook\n",
    "# it was described in the previous chapter of the book\n",
    "# when we discussed feature extraction\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "paths = ['war_and_peace.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer...\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize a tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "print('Training tokenizer...')\n",
    "\n",
    "# Customize training\n",
    "# we use a large vocabulary size, but we could also do with ca. 10_000\n",
    "tokenizer.train(files=paths, \n",
    "                vocab_size=52_000, \n",
    "                min_frequency=2, \n",
    "                special_tokens=[\"<s>\",\"<pad>\",\"</s>\",\"<unk>\",\"<mask>\",])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[55, 401, 69, 4628, 1448, 384, 1739, 283, 1891]\n"
     ]
    }
   ],
   "source": [
    "# check how the tokenizer works for this string \"int main() { return 0; }\"\n",
    "encoded = tokenizer.encode(\"Santa Claus is coming to town\")\n",
    "\n",
    "print(encoded.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nqYKX1XYyRI-",
    "outputId": "8e2f0316-455d-443b-ca8b-146bdd2e3a6a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['brownieBERTa/vocab.json', 'brownieBERTa/merges.txt']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# we give this model a catchy name - wolfBERTa\n",
    "# because it is a RoBERTa model trained on the WolfSSL source code\n",
    "token_dir = './brownieBERTa'\n",
    "\n",
    "if not os.path.exists(token_dir):\n",
    "  os.makedirs(token_dir)\n",
    "\n",
    "tokenizer.save_model('brownieBERTa')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "3hlRcyWsQjVq"
   },
   "source": [
    "# Training the model\n",
    "\n",
    "Now, we can start preparing to train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "hO5M3vrAhcuj"
   },
   "outputs": [],
   "source": [
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "# let's make sure that the tokenizer does not provide more tokens than we expect\n",
    "# we expect 510 tokens, because we will use the BERT model\n",
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=148)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "LTXXutqeDzPi"
   },
   "outputs": [],
   "source": [
    "# import the RoBERTa configuration\n",
    "from transformers import RobertaConfig\n",
    "\n",
    "# initialize the configuration\n",
    "# please note that the vocab size is the same as the one in the tokenizer. \n",
    "# if it is not, we could get exceptions that the model and the tokenizer are not compatible\n",
    "config = RobertaConfig(\n",
    "    vocab_size=52_000,\n",
    "    max_position_embeddings=512,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BzMqR-dzF4Ro",
    "outputId": "b43302a1-723d-4126-eb82-3bcb2f3fc584",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initializing a Model From Scratch\n",
    "from transformers import RobertaForMaskedLM\n",
    "\n",
    "# initialize the model\n",
    "model = RobertaForMaskedLM(config=config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the dataset for training\n",
    "\n",
    "We use the datasets library from Hugging Face in order  to load the dataset. It allows us to work with larger datasets and in a more efficient way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but before we actually train the model\n",
    "# we need to change the tokenizer to the one that we trained\n",
    "# and to make it compatible with the tokenizer that is expected by the model\n",
    "# so we read it from the file under a different tokenizer\n",
    "from transformers import RobertaTokenizer\n",
    "\n",
    "# initialize the tokenizer from the file\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"./brownieBERTa\", max_length=510)\n",
    "\n",
    "# please note that if we use a tokenizer that was trained before\n",
    "# the vanilla version of BPETokenizer, we will get an exception\n",
    "# that the BPE tokenizer is not collable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a25aacbe6504d1fb05de427ba941380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's see if we can change this to use the Dataset library instead of the transformers\n",
    "from datasets import load_dataset\n",
    "\n",
    "new_dataset = load_dataset(\"text\", data_files='./war_and_peace.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70f70993d1f743bd831c6f5d11c2e591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/49767 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# now, let's tokenize the dataset\n",
    "\n",
    "# num_proc is the argument to use all cores\n",
    "tokenized_dataset = new_dataset.map(lambda x: tokenizer(x[\"text\"]), num_proc=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "zTgWPa9Dipk2"
   },
   "outputs": [],
   "source": [
    "# training of the model requires a data collator\n",
    "# which creates a random set of tokens to mask\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "YpvnFFmZJD-N"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miroslaw/venvs/standards_n6/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9750' max='9750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9750/9750 35:03, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>6.821000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>5.714800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>5.168800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.812600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>4.567600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>4.412600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>4.279500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>4.168800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>4.059500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>3.976300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>3.911500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>3.849900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>3.795600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>3.753400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>3.696700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>3.692100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>3.656400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>3.629400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>3.629000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9750, training_loss=4.277055413661859, metrics={'train_runtime': 2103.8946, 'train_samples_per_second': 1182.735, 'train_steps_per_second': 4.634, 'total_flos': 1.5609333240384e+16, 'train_loss': 4.277055413661859, 'epoch': 50.0})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now, we can train the model\n",
    "# by creating the trainer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./brownieBERTa\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=50,\n",
    "    per_device_train_batch_size=256,\n",
    "    save_steps=1_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    ")\n",
    "\n",
    "# start the training process by calling the train method\n",
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "B-2hD_OLSa81"
   },
   "source": [
    "# Save the final model to hard drive\n",
    "\n",
    "Finally, we save the model to the hard drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QDNgPls7_l13",
    "outputId": "b86b8d70-2178-4a2e-bf09-ab417925698e"
   },
   "outputs": [],
   "source": [
    "trainer.save_model(\"./brownieBERTa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model\n",
    "\n",
    "Now, let's test the model -- predict one token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.3422628343105316,\n",
      "  'sequence': 'Santa Claus.”',\n",
      "  'token': 415,\n",
      "  'token_str': '.”'},\n",
      " {'score': 0.23653315007686615,\n",
      "  'sequence': 'Santa Claus.',\n",
      "  'token': 18,\n",
      "  'token_str': '.'},\n",
      " {'score': 0.17522436380386353,\n",
      "  'sequence': 'Santa Claus!”',\n",
      "  'token': 419,\n",
      "  'token_str': '!”'},\n",
      " {'score': 0.04776066541671753,\n",
      "  'sequence': 'Santa Claus?”',\n",
      "  'token': 460,\n",
      "  'token_str': '?”'},\n",
      " {'score': 0.04432617872953415,\n",
      "  'sequence': 'Santa Claus....”',\n",
      "  'token': 1550,\n",
      "  'token_str': '....”'},\n",
      " {'score': 0.03472210839390755,\n",
      "  'sequence': 'Santa Claus...”',\n",
      "  'token': 799,\n",
      "  'token_str': '...”'},\n",
      " {'score': 0.015623221173882484,\n",
      "  'sequence': 'Santa Claus!...”',\n",
      "  'token': 2791,\n",
      "  'token_str': '!...”'},\n",
      " {'score': 0.011783414520323277,\n",
      "  'sequence': 'Santa Claus:',\n",
      "  'token': 30,\n",
      "  'token_str': ':'},\n",
      " {'score': 0.008496532216668129,\n",
      "  'sequence': 'Santa Claus?...”',\n",
      "  'token': 3404,\n",
      "  'token_str': '?...”'},\n",
      " {'score': 0.0035278876312077045,\n",
      "  'sequence': 'Santa Claus.)',\n",
      "  'token': 2705,\n",
      "  'token_str': '.)'}]\n"
     ]
    }
   ],
   "source": [
    "# make a prediction\n",
    "from transformers import pipeline\n",
    "from pprint import pprint\n",
    "\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=\"./brownieBERTa\",\n",
    "    tokenizer=\"./brownieBERTa\"\n",
    ")\n",
    "\n",
    "strPredicted = fill_mask(\"Santa Claus <mask>\", top_k=10)\n",
    "\n",
    "pprint(strPredicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction - embeddings\n",
    "\n",
    "In this place, we extract the embeddings from the model for each of the lines that we have in the training set. Then we add the line that we just tested int i = 0; to the list of lines.\n",
    "\n",
    "Then we visualize it using t-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ./brownieBERTa and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.1033674478530884,\n",
      " 1.2807064056396484,\n",
      " -0.7393246293067932,\n",
      " -0.75388103723526,\n",
      " 0.37746304273605347,\n",
      " 0.5539442896842957,\n",
      " -1.6205332279205322,\n",
      " 0.2173890918493271,\n",
      " -0.6585490107536316,\n",
      " 1.2766588926315308,\n",
      " 0.23506832122802734,\n",
      " -0.861558735370636,\n",
      " 0.40749630331993103,\n",
      " 2.3658220767974854,\n",
      " -0.3706724941730499,\n",
      " 0.7339725494384766,\n",
      " 0.022288169711828232,\n",
      " 0.45131033658981323,\n",
      " -0.363322913646698,\n",
      " -1.8899754285812378,\n",
      " 1.5935488939285278,\n",
      " -1.404867172241211,\n",
      " -1.4151560068130493,\n",
      " 0.14845840632915497,\n",
      " 0.4643068015575409,\n",
      " -1.8416754007339478,\n",
      " -1.6816915273666382,\n",
      " 1.4302928447723389,\n",
      " 0.06566373258829117,\n",
      " -0.3594614565372467,\n",
      " 2.1859748363494873,\n",
      " 1.8484517335891724,\n",
      " -0.6307582855224609,\n",
      " -1.6817268133163452,\n",
      " -0.06582394242286682,\n",
      " -1.5709772109985352,\n",
      " -0.7745420932769775,\n",
      " 0.049157850444316864,\n",
      " -0.5926477909088135,\n",
      " -0.5225352644920349,\n",
      " -1.5673067569732666,\n",
      " 0.8782007098197937,\n",
      " 0.24394680559635162,\n",
      " -0.4026496708393097,\n",
      " -0.25952962040901184,\n",
      " -2.838500738143921,\n",
      " 0.4026079773902893,\n",
      " 0.8456125855445862,\n",
      " -0.0315823070704937,\n",
      " -0.9560059905052185,\n",
      " 0.42499464750289917,\n",
      " 0.25661540031433105,\n",
      " 0.5080335736274719,\n",
      " 0.8407230973243713,\n",
      " -0.1564318984746933,\n",
      " 1.9780199527740479,\n",
      " -0.2605034112930298,\n",
      " 1.2429163455963135,\n",
      " -0.27334773540496826,\n",
      " 0.8862937092781067,\n",
      " 0.9532507658004761,\n",
      " -0.7964964509010315,\n",
      " 2.2138638496398926,\n",
      " -0.6167751550674438,\n",
      " -1.0543268918991089,\n",
      " 0.9341490864753723,\n",
      " -1.573327660560608,\n",
      " -0.1844872087240219,\n",
      " -0.31969794631004333,\n",
      " 2.2462387084960938,\n",
      " 1.3784250020980835,\n",
      " 0.209846630692482,\n",
      " -0.08429370075464249,\n",
      " 0.2865588366985321,\n",
      " -1.6045737266540527,\n",
      " -1.122000813484192,\n",
      " 0.23888930678367615,\n",
      " -0.7897419333457947,\n",
      " 2.173248529434204,\n",
      " -0.5560537576675415,\n",
      " -0.19698606431484222,\n",
      " -0.13385717570781708,\n",
      " 2.067981243133545,\n",
      " 0.8508477807044983,\n",
      " 1.3429275751113892,\n",
      " -1.638608455657959,\n",
      " -0.9635579586029053,\n",
      " 0.5963086485862732,\n",
      " 0.798811137676239,\n",
      " 0.6199915409088135,\n",
      " 0.3234182894229889,\n",
      " -0.17590251564979553,\n",
      " -0.06794028729200363,\n",
      " -0.09665979444980621,\n",
      " 0.7010001540184021,\n",
      " 0.8310605883598328,\n",
      " -0.4512307941913605,\n",
      " 0.16383041441440582,\n",
      " -0.5901361703872681,\n",
      " 1.6126410961151123,\n",
      " 0.5404084324836731,\n",
      " 0.008252250030636787,\n",
      " 0.7059657573699951,\n",
      " 0.6909739971160889,\n",
      " -0.2899225652217865,\n",
      " 0.1728055775165558,\n",
      " 0.57528156042099,\n",
      " -0.20092931389808655,\n",
      " -1.8391683101654053,\n",
      " 1.234865427017212,\n",
      " -0.4391052722930908,\n",
      " 0.010149318724870682,\n",
      " -0.18153533339500427,\n",
      " 1.063583493232727,\n",
      " 1.4449032545089722,\n",
      " -0.32292211055755615,\n",
      " -0.33593663573265076,\n",
      " -0.062416985630989075,\n",
      " -0.21318219602108002,\n",
      " -0.5832604765892029,\n",
      " -1.372199296951294,\n",
      " -1.235671043395996,\n",
      " -0.1669028252363205,\n",
      " -2.1363844871520996,\n",
      " 0.6392323970794678,\n",
      " 1.2389823198318481,\n",
      " -2.218909502029419,\n",
      " 0.6746235489845276,\n",
      " 1.3388606309890747,\n",
      " -1.2123289108276367,\n",
      " 0.5490990877151489,\n",
      " -0.7552400231361389,\n",
      " -0.9980580806732178,\n",
      " 0.7264349460601807,\n",
      " 0.09414012730121613,\n",
      " 0.4882136881351471,\n",
      " 2.8382668495178223,\n",
      " 0.8692205548286438,\n",
      " 0.9173907041549683,\n",
      " 1.6654694080352783,\n",
      " 0.4769648313522339,\n",
      " 0.13535939157009125,\n",
      " 0.19660477340221405,\n",
      " 0.2804670035839081,\n",
      " 0.28646722435951233,\n",
      " 1.8611315488815308,\n",
      " 1.2308064699172974,\n",
      " -0.33459579944610596,\n",
      " 1.1833093166351318,\n",
      " 0.5526837706565857,\n",
      " 0.24424998462200165,\n",
      " -0.13426873087882996,\n",
      " -1.4596071243286133,\n",
      " -0.6403335928916931,\n",
      " -0.812976598739624,\n",
      " -0.6934536695480347,\n",
      " 0.7516090273857117,\n",
      " -0.6252285838127136,\n",
      " 1.008689284324646,\n",
      " 0.038494210690259933,\n",
      " -0.10714606940746307,\n",
      " 0.4610796868801117,\n",
      " 0.1223565861582756,\n",
      " 0.1283784806728363,\n",
      " 1.5545666217803955,\n",
      " -0.06360337883234024,\n",
      " 0.7583498358726501,\n",
      " 1.0764501094818115,\n",
      " 1.5817971229553223,\n",
      " 0.7994385361671448,\n",
      " 0.15954841673374176,\n",
      " -0.11557133495807648,\n",
      " 0.24373449385166168,\n",
      " -0.9805583953857422,\n",
      " -0.7891397476196289,\n",
      " 0.974705159664154,\n",
      " -0.3176984190940857,\n",
      " -0.2297714203596115,\n",
      " 1.510599970817566,\n",
      " -0.6599346995353699,\n",
      " -0.596942126750946,\n",
      " 1.3927606344223022,\n",
      " -0.23204609751701355,\n",
      " -1.5523152351379395,\n",
      " -1.178628921508789,\n",
      " 0.2514558434486389,\n",
      " -0.8993580341339111,\n",
      " 0.03614988550543785,\n",
      " -0.12099017947912216,\n",
      " 0.041995350271463394,\n",
      " -0.35707032680511475,\n",
      " 0.028147950768470764,\n",
      " -1.009596824645996,\n",
      " -0.2642892003059387,\n",
      " -0.457040399312973,\n",
      " -0.06302303820848465,\n",
      " 1.1120578050613403,\n",
      " 0.7635451555252075,\n",
      " 0.9240307211875916,\n",
      " -1.548599123954773,\n",
      " 0.08498656004667282,\n",
      " -0.1272355169057846,\n",
      " -1.2110590934753418,\n",
      " 1.410436749458313,\n",
      " -0.3162768483161926,\n",
      " -1.0811216831207275,\n",
      " -1.4973812103271484,\n",
      " -0.052614737302064896,\n",
      " 0.6087894439697266,\n",
      " -0.9122427105903625,\n",
      " 1.1999434232711792,\n",
      " -0.3857254385948181,\n",
      " 1.4444345235824585,\n",
      " -0.3580772280693054,\n",
      " -1.100626826286316,\n",
      " -0.7580552101135254,\n",
      " 1.0517038106918335,\n",
      " 0.6299863457679749,\n",
      " 0.6336345076560974,\n",
      " -0.27522948384284973,\n",
      " 0.5005708932876587,\n",
      " -0.7043170928955078,\n",
      " -0.204685777425766,\n",
      " 0.13183395564556122,\n",
      " -0.45902881026268005,\n",
      " -1.2555018663406372,\n",
      " -0.28288817405700684,\n",
      " 2.473079204559326,\n",
      " 0.38634222745895386,\n",
      " 0.9277220368385315,\n",
      " -0.018451154232025146,\n",
      " -1.546811580657959,\n",
      " -2.013098955154419,\n",
      " -0.14305952191352844,\n",
      " -1.5779151916503906,\n",
      " 0.25977736711502075,\n",
      " -0.1535198837518692,\n",
      " -0.7221682071685791,\n",
      " 0.5782461166381836,\n",
      " -0.5622778534889221,\n",
      " -0.0679195299744606,\n",
      " 1.1645439863204956,\n",
      " -0.20162099599838257,\n",
      " -0.1648147702217102,\n",
      " -1.1882929801940918,\n",
      " -1.7265394926071167,\n",
      " 0.09989871084690094,\n",
      " 1.5075228214263916,\n",
      " 0.400028795003891,\n",
      " -0.03203379362821579,\n",
      " 0.5233718752861023,\n",
      " 0.0014074415666982532,\n",
      " -1.047526240348816,\n",
      " -0.7540208101272583,\n",
      " -0.11425969749689102,\n",
      " 0.8108697533607483,\n",
      " -0.45133569836616516,\n",
      " -0.29076430201530457,\n",
      " 0.3055322766304016,\n",
      " -0.7813669443130493,\n",
      " 0.7928497791290283,\n",
      " 0.36548393964767456,\n",
      " -1.4219162464141846,\n",
      " 1.731337070465088,\n",
      " 0.3002753257751465,\n",
      " -3.10695743560791,\n",
      " -0.6830188632011414,\n",
      " -1.7264200448989868,\n",
      " -1.2374541759490967,\n",
      " 1.2861793041229248,\n",
      " 0.07812895625829697,\n",
      " -0.17429733276367188,\n",
      " -1.5346474647521973,\n",
      " 0.7600179314613342,\n",
      " -0.5399492383003235,\n",
      " 1.7246612310409546,\n",
      " 1.3708816766738892,\n",
      " 0.09253944456577301,\n",
      " 2.3435275554656982,\n",
      " 1.7278811931610107,\n",
      " 0.08679146319627762,\n",
      " 0.39903226494789124,\n",
      " 0.3089381158351898,\n",
      " -0.023148732259869576,\n",
      " -0.6379653215408325,\n",
      " -0.8197733759880066,\n",
      " 1.39948308467865,\n",
      " 0.16664347052574158,\n",
      " 1.1109743118286133,\n",
      " -0.23564976453781128,\n",
      " -1.2518060207366943,\n",
      " 0.01194925606250763,\n",
      " -0.027060097083449364,\n",
      " 0.009530577808618546,\n",
      " -0.1576603204011917,\n",
      " 0.6136696338653564,\n",
      " -0.4350297749042511,\n",
      " -0.01351249124854803,\n",
      " 1.261819839477539,\n",
      " 0.882108211517334,\n",
      " -0.057888004928827286,\n",
      " -0.11102023720741272,\n",
      " 1.0723975896835327,\n",
      " 0.9409846067428589,\n",
      " -0.3675173819065094,\n",
      " 0.5024523138999939,\n",
      " 0.38561055064201355,\n",
      " -0.25351348519325256,\n",
      " 1.9296785593032837,\n",
      " 0.9351328015327454,\n",
      " -2.0417675971984863,\n",
      " 2.6786582469940186,\n",
      " -0.5955554842948914,\n",
      " -0.6932835578918457,\n",
      " -0.045578524470329285,\n",
      " -0.9417601227760315,\n",
      " 0.3474392890930176,\n",
      " 0.06171567738056183,\n",
      " -0.769633948802948,\n",
      " 0.5920765399932861,\n",
      " -1.8525680303573608,\n",
      " -1.7153421640396118,\n",
      " 0.9119595885276794,\n",
      " -0.05575118586421013,\n",
      " -1.3827040195465088,\n",
      " 0.07862761616706848,\n",
      " 0.8382969498634338,\n",
      " 0.11787223815917969,\n",
      " -0.3868848979473114,\n",
      " 2.228943109512329,\n",
      " 0.9200594425201416,\n",
      " 0.5115218162536621,\n",
      " 0.41153284907341003,\n",
      " -0.15506459772586823,\n",
      " 0.4118913412094116,\n",
      " 0.3075452148914337,\n",
      " 0.4760836958885193,\n",
      " -0.6202393174171448,\n",
      " 0.7263585925102234,\n",
      " -1.3660247325897217,\n",
      " -0.5952128171920776,\n",
      " -1.2218354940414429,\n",
      " 0.9993007779121399,\n",
      " -0.19620613753795624,\n",
      " 0.29659605026245117,\n",
      " -0.2910080850124359,\n",
      " 0.6976063251495361,\n",
      " 1.665902853012085,\n",
      " 0.46313953399658203,\n",
      " -1.1037721633911133,\n",
      " -0.5939058661460876,\n",
      " -0.5816546678543091,\n",
      " -1.9334841966629028,\n",
      " 0.5254851579666138,\n",
      " 1.5977261066436768,\n",
      " 1.0976320505142212,\n",
      " -0.3061641454696655,\n",
      " -2.8689470291137695,\n",
      " -1.7270967960357666,\n",
      " -1.494936466217041,\n",
      " -0.9484963417053223,\n",
      " -0.16863998770713806,\n",
      " 0.3365679383277893,\n",
      " -1.575753092765808,\n",
      " 0.45447641611099243,\n",
      " 0.40022850036621094,\n",
      " 1.0908305644989014,\n",
      " 0.31629034876823425,\n",
      " 1.1448534727096558,\n",
      " 0.7824010848999023,\n",
      " -0.4409470558166504,\n",
      " 0.6888468265533447,\n",
      " -0.2676011621952057,\n",
      " -0.6264088153839111,\n",
      " -0.0880885049700737,\n",
      " -1.294590711593628,\n",
      " -0.8234207034111023,\n",
      " 0.40381190180778503,\n",
      " 0.5762673020362854,\n",
      " -0.1346707046031952,\n",
      " 1.1029144525527954,\n",
      " 0.22666653990745544,\n",
      " 1.3163055181503296,\n",
      " 0.24769964814186096,\n",
      " 0.48469415307044983,\n",
      " 0.6344494223594666,\n",
      " -0.00880667008459568,\n",
      " -0.544888973236084,\n",
      " 0.2716250717639923,\n",
      " 1.063002109527588,\n",
      " -1.3101904392242432,\n",
      " 0.3732600808143616,\n",
      " 1.5166494846343994,\n",
      " 0.11613735556602478,\n",
      " 0.222296804189682,\n",
      " -0.772700309753418,\n",
      " -1.143235445022583,\n",
      " -0.07286278158426285,\n",
      " 0.3904494643211365,\n",
      " 0.6394157409667969,\n",
      " -0.7429218292236328,\n",
      " 0.42153504490852356,\n",
      " -0.21096082031726837,\n",
      " 0.14744839072227478,\n",
      " 1.8525009155273438,\n",
      " 0.18660494685173035,\n",
      " 1.008661150932312,\n",
      " -0.3176445960998535,\n",
      " -0.9910740256309509,\n",
      " -1.0312011241912842,\n",
      " 0.8106544613838196,\n",
      " -0.5654820203781128,\n",
      " -1.2864725589752197,\n",
      " -0.34577563405036926,\n",
      " 0.6249094009399414,\n",
      " 1.770413875579834,\n",
      " -0.1311895251274109,\n",
      " 1.0294722318649292,\n",
      " -0.047892145812511444,\n",
      " 0.6277171969413757,\n",
      " -0.6221939921379089,\n",
      " 1.8040977716445923,\n",
      " 0.1688677966594696,\n",
      " -0.002298008883371949,\n",
      " 0.1911543905735016,\n",
      " -1.1097995042800903,\n",
      " 0.20554785430431366,\n",
      " 1.9714932441711426,\n",
      " 0.8865795135498047,\n",
      " -0.42289814352989197,\n",
      " 1.105904459953308,\n",
      " -0.7763413190841675,\n",
      " 1.328479290008545,\n",
      " -0.5126344561576843,\n",
      " 1.6834152936935425,\n",
      " -0.5703155994415283,\n",
      " -0.9316086173057556,\n",
      " -0.8963210582733154,\n",
      " 1.2414286136627197,\n",
      " -0.4890922009944916,\n",
      " 1.1217888593673706,\n",
      " -1.1638380289077759,\n",
      " 0.6715154647827148,\n",
      " -0.3012380003929138,\n",
      " -0.09137670695781708,\n",
      " 1.246801733970642,\n",
      " -0.21740058064460754,\n",
      " -0.25204843282699585,\n",
      " -0.9563717246055603,\n",
      " 1.2323647737503052,\n",
      " 0.024162186309695244,\n",
      " 1.7515908479690552,\n",
      " -0.7159436345100403,\n",
      " -1.1280455589294434,\n",
      " -2.2847635746002197,\n",
      " -0.1327444612979889,\n",
      " -0.46731433272361755,\n",
      " -0.4489763081073761,\n",
      " -1.0031489133834839,\n",
      " -1.138981580734253,\n",
      " -1.2662382125854492,\n",
      " -0.13901251554489136,\n",
      " 0.32907000184059143,\n",
      " -0.9557263255119324,\n",
      " 0.1573134958744049,\n",
      " 0.9337202906608582,\n",
      " -1.1971999406814575,\n",
      " 0.04774021729826927,\n",
      " -0.3759506642818451,\n",
      " 0.1313755065202713,\n",
      " -0.7454311847686768,\n",
      " 0.18241828680038452,\n",
      " -0.49963992834091187,\n",
      " 2.0391359329223633,\n",
      " 0.2059268057346344,\n",
      " -0.5067926049232483,\n",
      " 0.23185023665428162,\n",
      " 0.23683828115463257,\n",
      " 0.7362332940101624,\n",
      " -1.7051752805709839,\n",
      " 0.3353767693042755,\n",
      " 0.43130865693092346,\n",
      " -1.1188610792160034,\n",
      " -0.8658299446105957,\n",
      " -0.5630449056625366,\n",
      " 0.8390920758247375,\n",
      " 1.9617966413497925,\n",
      " -1.216715931892395,\n",
      " -0.6354964971542358,\n",
      " -0.8277422785758972,\n",
      " 1.40007746219635,\n",
      " -2.2304043769836426,\n",
      " 0.6759933233261108,\n",
      " -2.901174783706665,\n",
      " 0.0183696411550045,\n",
      " -0.13230298459529877,\n",
      " -2.4806411266326904,\n",
      " 0.7113600373268127,\n",
      " -0.14121833443641663,\n",
      " 2.1205015182495117,\n",
      " 1.2726842164993286,\n",
      " -0.1934090107679367,\n",
      " 1.1772819757461548,\n",
      " 0.7605692744255066,\n",
      " 2.034403085708618,\n",
      " -0.5301098227500916,\n",
      " -0.351802796125412,\n",
      " -0.7482805252075195,\n",
      " 0.6034453511238098,\n",
      " 0.8806755542755127,\n",
      " 0.08544932305812836,\n",
      " 2.6154634952545166,\n",
      " -0.4746876358985901,\n",
      " -0.9076763391494751,\n",
      " 0.6618825793266296,\n",
      " -0.835922122001648,\n",
      " -1.511817216873169,\n",
      " -0.40063050389289856,\n",
      " -1.7480809688568115,\n",
      " 0.050666194409132004,\n",
      " -1.0730637311935425,\n",
      " -2.435943126678467,\n",
      " 1.7934879064559937,\n",
      " 0.6622849702835083,\n",
      " 1.0016372203826904,\n",
      " -0.5060285925865173,\n",
      " -0.5764369964599609,\n",
      " -0.0796421468257904,\n",
      " -1.0050806999206543,\n",
      " -1.048186182975769,\n",
      " 0.7976488471031189,\n",
      " -0.18538235127925873,\n",
      " 1.9790810346603394,\n",
      " 0.5181501507759094,\n",
      " -0.8449307084083557,\n",
      " 0.5834635496139526,\n",
      " 1.237258791923523,\n",
      " -0.001036047819070518,\n",
      " 1.1264654397964478,\n",
      " -0.9774914979934692,\n",
      " 0.7712605595588684,\n",
      " -0.882344663143158,\n",
      " 1.2942441701889038,\n",
      " -0.23851612210273743,\n",
      " -0.5497985482215881,\n",
      " 1.0071241855621338,\n",
      " 1.2458373308181763,\n",
      " -2.4510300159454346,\n",
      " 1.649194359779358,\n",
      " -1.835240364074707,\n",
      " 0.4717912971973419,\n",
      " 1.4703116416931152,\n",
      " 0.5435160398483276,\n",
      " -1.1261473894119263,\n",
      " -0.6602212190628052,\n",
      " -1.122265338897705,\n",
      " -0.6820888519287109,\n",
      " -0.2872765362262726,\n",
      " 0.39112383127212524,\n",
      " -1.7024368047714233,\n",
      " -1.5203328132629395,\n",
      " -0.5903673768043518,\n",
      " 0.13572336733341217,\n",
      " 0.5841820240020752,\n",
      " 0.6742556691169739,\n",
      " -1.0884673595428467,\n",
      " -1.0223846435546875,\n",
      " 1.2720059156417847,\n",
      " -1.3141976594924927,\n",
      " 0.3851013481616974,\n",
      " -0.8938699960708618,\n",
      " -0.1278105527162552,\n",
      " 1.0256515741348267,\n",
      " 0.5466023087501526,\n",
      " 0.7058173418045044,\n",
      " -0.9465104937553406,\n",
      " -1.0314193964004517,\n",
      " -1.1429814100265503,\n",
      " -0.07399020344018936,\n",
      " 0.18031379580497742,\n",
      " -0.42645031213760376,\n",
      " -1.5376335382461548,\n",
      " 0.05314258113503456,\n",
      " 0.768703818321228,\n",
      " 1.60762619972229,\n",
      " 3.006281852722168,\n",
      " 1.006716251373291,\n",
      " 0.20497065782546997,\n",
      " -0.08708850294351578,\n",
      " 0.4181414544582367,\n",
      " -1.386812448501587,\n",
      " -0.42636778950691223,\n",
      " 1.6210434436798096,\n",
      " -0.4235793352127075,\n",
      " 0.8766927719116211,\n",
      " 0.4611770808696747,\n",
      " -1.027768850326538,\n",
      " -0.90986168384552,\n",
      " -0.287562757730484,\n",
      " -0.2067357748746872,\n",
      " 0.4486517012119293,\n",
      " -2.4453954696655273,\n",
      " 1.1880515813827515,\n",
      " 1.010382056236267,\n",
      " 1.5451505184173584,\n",
      " -0.22424271702766418,\n",
      " 1.072923183441162,\n",
      " -0.2335466891527176,\n",
      " -0.06527205556631088,\n",
      " 1.3194137811660767,\n",
      " -0.1768086701631546,\n",
      " 0.7035638093948364,\n",
      " -0.7687543034553528,\n",
      " 0.2990933954715729,\n",
      " 0.5275107622146606,\n",
      " -1.7227156162261963,\n",
      " -1.3876758813858032,\n",
      " 0.2709107995033264,\n",
      " -0.3967491686344147,\n",
      " 1.1814411878585815,\n",
      " 0.2078591287136078,\n",
      " 0.07981499284505844,\n",
      " -2.6347484588623047,\n",
      " 0.6416204571723938,\n",
      " 0.2790299952030182,\n",
      " 1.0546025037765503,\n",
      " 1.1819978952407837,\n",
      " -0.11613999307155609,\n",
      " -0.23040352761745453,\n",
      " 0.3152761161327362,\n",
      " 0.8402419090270996,\n",
      " -0.6147114038467407,\n",
      " 0.48980000615119934,\n",
      " 1.5038859844207764,\n",
      " 0.5467560291290283,\n",
      " -1.4309186935424805,\n",
      " -0.3302944302558899,\n",
      " 0.13334114849567413,\n",
      " 0.5672135353088379,\n",
      " -0.8289042711257935,\n",
      " -0.5234032869338989,\n",
      " 1.3429968357086182,\n",
      " -1.1992814540863037,\n",
      " 0.20674198865890503,\n",
      " 0.14783981442451477,\n",
      " -2.3770575523376465,\n",
      " 1.2514370679855347,\n",
      " 0.2606808841228485,\n",
      " 0.013763857074081898,\n",
      " -0.8238496780395508,\n",
      " -0.7743053436279297,\n",
      " -0.9370853304862976,\n",
      " -1.1622363328933716,\n",
      " -1.0310146808624268,\n",
      " -0.95857173204422,\n",
      " -1.6197510957717896,\n",
      " -0.13561441004276276,\n",
      " -0.27892494201660156,\n",
      " -0.7036160230636597,\n",
      " 1.0096163749694824,\n",
      " -0.11326264590024948,\n",
      " 1.2922406196594238,\n",
      " 1.1074883937835693,\n",
      " -0.6944711208343506,\n",
      " 0.6241744756698608,\n",
      " 1.3086910247802734,\n",
      " 0.07441364228725433,\n",
      " 0.10379423946142197,\n",
      " 0.9093371033668518,\n",
      " -0.676408052444458,\n",
      " -1.7950509786605835,\n",
      " 1.4192205667495728,\n",
      " 0.6418012380599976,\n",
      " -1.2091535329818726,\n",
      " 0.8612704277038574,\n",
      " -0.6082570552825928,\n",
      " -0.014882187359035015,\n",
      " -0.6839814782142639,\n",
      " 0.05317464470863342,\n",
      " -1.3106260299682617,\n",
      " 0.007747218944132328,\n",
      " -0.8393025398254395,\n",
      " -0.42494475841522217,\n",
      " -1.1583632230758667,\n",
      " -0.2713184952735901,\n",
      " 0.6488080024719238,\n",
      " 0.04422268271446228,\n",
      " -1.212607502937317,\n",
      " 0.2421855926513672,\n",
      " -0.8484598994255066,\n",
      " 0.4201818108558655,\n",
      " 0.02829694002866745,\n",
      " -0.8874775767326355,\n",
      " -0.2639952301979065,\n",
      " -3.399472236633301,\n",
      " 0.7049060463905334,\n",
      " -0.7755149602890015,\n",
      " -0.6568971872329712,\n",
      " 0.4509831666946411,\n",
      " 0.4646146297454834,\n",
      " -1.0251487493515015,\n",
      " -0.45543017983436584,\n",
      " 1.6000256538391113,\n",
      " 0.6118250489234924,\n",
      " 0.12679484486579895,\n",
      " -0.4398593604564667,\n",
      " -0.8124712705612183,\n",
      " 1.056366205215454,\n",
      " -0.1271601915359497,\n",
      " 1.5444444417953491,\n",
      " 0.430408775806427,\n",
      " -1.0110458135604858,\n",
      " 1.2144885063171387,\n",
      " 0.7054651379585266,\n",
      " -0.9195650815963745,\n",
      " -2.8478012084960938,\n",
      " 0.7345945835113525,\n",
      " 0.612590491771698,\n",
      " 1.4819427728652954,\n",
      " -0.40204936265945435,\n",
      " -1.1771178245544434,\n",
      " -1.1550368070602417,\n",
      " 0.48032790422439575,\n",
      " 0.15592850744724274,\n",
      " -0.09737344831228256,\n",
      " 1.9215419292449951,\n",
      " -1.1828346252441406,\n",
      " -1.4489775896072388,\n",
      " -0.295016884803772,\n",
      " -0.6343576908111572,\n",
      " 0.2073686718940735,\n",
      " 0.6772392988204956,\n",
      " -1.4806443452835083,\n",
      " 1.4028005599975586,\n",
      " -0.8166168332099915,\n",
      " 0.28916215896606445,\n",
      " 0.05658063665032387,\n",
      " -0.11762990057468414,\n",
      " -0.8916454911231995,\n",
      " 0.36396369338035583,\n",
      " -0.5596964955329895,\n",
      " 0.4974450170993805,\n",
      " 0.9179940819740295,\n",
      " 0.5567457675933838,\n",
      " -2.0147151947021484,\n",
      " -0.09858500957489014,\n",
      " -0.3204899728298187,\n",
      " 1.0170152187347412,\n",
      " 0.04845763370394707,\n",
      " -0.0035918308421969414,\n",
      " 0.9885596632957458,\n",
      " 0.24493508040905,\n",
      " -1.7472673654556274,\n",
      " -0.47514379024505615,\n",
      " 0.04141615331172943,\n",
      " -1.0802158117294312,\n",
      " 0.2880678176879883,\n",
      " 0.5743158459663391,\n",
      " -0.1625259667634964,\n",
      " -0.03754670172929764,\n",
      " -0.320451557636261,\n",
      " -0.23757679760456085,\n",
      " -1.204991102218628,\n",
      " 1.5976394414901733,\n",
      " -0.9122999310493469,\n",
      " 1.3145246505737305,\n",
      " 1.1018249988555908,\n",
      " -0.014521585777401924]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from pprint import pprint\n",
    "\n",
    "feature_extraction = pipeline(\n",
    "    \"feature-extraction\",\n",
    "    model=\"./brownieBERTa\",\n",
    "    tokenizer=\"./brownieBERTa\"\n",
    ")\n",
    "\n",
    "strPredicted = feature_extraction(\"Brownie is good.\")\n",
    "\n",
    "pprint(strPredicted[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ./brownieBERTa and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 49768/49768 [10:30<00:00, 78.87it/s] \n"
     ]
    }
   ],
   "source": [
    "# now, go through the file c_declarations.c and extract features for each of the lines\n",
    "\n",
    "# we will use the same tokenizer as before\n",
    "# but we will use the feature extraction pipeline\n",
    "# to extract features for each of the lines in the file\n",
    "from transformers import pipeline\n",
    "\n",
    "feature_extraction = pipeline(\n",
    "    \"feature-extraction\",\n",
    "    model=\"./brownieBERTa\",\n",
    "    tokenizer=\"./brownieBERTa\"\n",
    ")\n",
    "\n",
    "# read the file with all the lines used to train the model\n",
    "with open('./war_and_peace.txt', 'r') as file:\n",
    "    data = file.readlines()\n",
    "\n",
    "data.append(\"Santa Claus is coming to town\")\n",
    "\n",
    "# extract features for each of the lines\n",
    "from tqdm import tqdm\n",
    "\n",
    "features = [feature_extraction(line)[0][0] for line in tqdm(data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert features to a dataframe\n",
    "import pandas as pd \n",
    "\n",
    "df = pd.DataFrame(features)\n",
    "\n",
    "df['type'] = 0\n",
    "\n",
    "df.index = data\n",
    "\n",
    "# if the index is int i = 0; then the type is 1\n",
    "df.loc[\"Santa Claus is coming to town\", 'type'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFeatures = df.drop(columns=['type'], axis=1)\n",
    "dfFeatures.dropna(inplace=True)\n",
    "\n",
    "dfFeatures.to_csv('features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now visualize the features\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def visualize(perplexity):\n",
    "    # initialize the t-SNE\n",
    "    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)\n",
    "\n",
    "    # fit the t-SNE\n",
    "    X_embedded = tsne.fit_transform(dfFeatures)\n",
    "\n",
    "    # Create a color map based on the values in the first column\n",
    "    colors = ['blue' if val == 0.0 else 'red' for val in df['type']]\n",
    "\n",
    "    # plot the t-SNE\n",
    "    plt.figure(figsize=(18, 12))\n",
    "    plt.scatter(X_embedded[:, 0], X_embedded[:, 1], c=colors)\n",
    "    #plt.xlabel('Component 1', fontsize=20)\n",
    "    #plt.ylabel('Component 2', fontsize=20)\n",
    "    #plt.xticks(fontsize=16)\n",
    "    #plt.yticks(fontsize=16)\n",
    "\n",
    "    # Format the tick labels to 3 decimal places\n",
    "    #plt.gca().xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x:.3f}'))\n",
    "    #plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y:.3f}'))\n",
    "\n",
    "    plt.savefig(f'./img/war_and_peace_tsne_{perplexity}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for i in tqdm(range(1, 5)):\n",
    "    visualize(i)\n",
    "\n",
    "\n",
    "#len(features)):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning\n",
    "\n",
    "In this part of the tutorial, we fine tune our model to predict declarations of functions. We use kNN classifier for that reason.\n",
    "\n",
    "There are two steps that we need to do:\n",
    "1) add another column to the dataframe df, where we add information about if a declaration is a function\n",
    "2) we train and validate the kNN classifier for that task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now add another column to df\n",
    "# if the index contains \"(\" then the type is 2.\n",
    "\n",
    "df.loc[df.index.str.contains(\"Santa\"), 'type'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the training set: 0.9999799067674008\n",
      "The type of the line is: [0]\n"
     ]
    }
   ],
   "source": [
    "# now train kNN to predict the type of the line\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# initialize the kNN\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# fit the kNN\n",
    "knn.fit(dfFeatures, df['type'])\n",
    "\n",
    "# predict the type of the line\n",
    "knn.predict(dfFeatures)\n",
    "\n",
    "# now validate the predictions\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "fAccuracy = accuracy_score(df[\"type\"], knn.predict(dfFeatures))\n",
    "\n",
    "print(f'Accuracy of the training set: {fAccuracy}')\n",
    "\n",
    "# now predict the type of the line \"int x = 12;\"\n",
    "\n",
    "# extract features for the line\n",
    "oneLineFeatures = feature_extraction(\"Is he coming? \")[0][0]\n",
    "\n",
    "# convert features to a dataframe\n",
    "dfLine = pd.DataFrame([oneLineFeatures])\n",
    "\n",
    "# predict the type of the line\n",
    "iType = knn.predict(dfLine)\n",
    "\n",
    "print(f'The type of the line is: {iType}')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this tutorial, we've learned how to train a simple transformer network. We downloaded the network's architecture from the HuggingFace hub, then we used our set of declarations of C variables and functions to train it.\n",
    "\n",
    "The result of this training is the network that can help us to write declarations of functions in C. \n",
    "\n",
    "If you want to dive deeper into this topic, please do the following exercises:\n",
    "1. For the fill-mask pipeline, change the predicted string to int main(int argc, <mask> *argv); observe whether the network can predict it; reflecton on the result. \n",
    "2. Reduce the number of training epochs to 1, train the network and check the suggestions; do the same for 10 epochs, 20, etc. Observe and reflect upon the quality of the suggestions.\n",
    "3. Go through the training set and add declarations that you are interested in, e.g., more variations of int main(....); train the network with 20-40 epochs and test your predictions. Observe how the suggestion changes if the network has a similar data in the training set and when it does not.\n",
    "4. In many places I used the vocabulary size of 5000, please change that to 100, train the network and observe what happens. You can even go further and change this vocabulary size to 10 and do the same. \n",
    "5. When training the tokenizer, I used the parameter min_frequency=2, please change that to 10, then go through the entire process and see how that impacts results. Do the same for min_frequency=1. Whis parameter gives the best prediction?\n",
    "6. Go through the code for visualization. Change the perplexity parameter of the t-SNE. Plot a few diagrams, reflect on how this parameter helps in understanding the data. "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "standards_n6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
