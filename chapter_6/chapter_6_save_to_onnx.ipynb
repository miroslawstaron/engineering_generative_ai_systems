{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "M1oqh0F6W3ad"
   },
   "source": [
    "# Saving a model to the ONNX format\n",
    "\n",
    "This demo illustrates how to save a model that we trained in chapter 1 to the ONNX format. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "aleXCJqVQPro"
   },
   "source": [
    "# Training a tokenizer\n",
    "\n",
    "The first step in training the model is to train the tokenizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just checking if CUDA is available on this computer\n",
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model from file\n",
    "\n",
    "Here, we load the model from the hard drive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from pprint import pprint\n",
    "\n",
    "feature_extraction = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=\"./decBERTa\",\n",
    "    tokenizer=\"./decBERTa\"\n",
    ")\n",
    "\n",
    "strPredicted = feature_extraction(\"int i = <mask>;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.9335035681724548,\n",
       " 'token': 266,\n",
       " 'token_str': ' 0',\n",
       " 'sequence': 'int i = 0;'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strPredicted[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import transformers\n",
    "from transformers.onnx import FeaturesManager\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_id = \"./decBERTa\"\n",
    "feature = \"masked-lm\"  \n",
    "model = AutoModelForMaskedLM.from_pretrained(model_id)  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Load config\n",
    "model_kind, model_onnx_config = FeaturesManager.check_supported_model_or_raise(model, feature=feature)\n",
    "onnx_config = model_onnx_config(model.config)\n",
    "\n",
    "# Export\n",
    "onnx_inputs, onnx_outputs = transformers.onnx.export(\n",
    "    preprocessor=tokenizer,\n",
    "    model=model,\n",
    "    config=onnx_config,\n",
    "    opset=13,\n",
    "    output=Path(\"decBERTa.onnx\")\n",
    ")\n",
    "\n",
    "print(\"ONNX model exported successfully to decBERTa.onnx\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
