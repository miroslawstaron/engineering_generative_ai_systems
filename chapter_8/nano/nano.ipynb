{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "353d22ef",
   "metadata": {},
   "source": [
    "# Testing nanoLLM from NVidia\n",
    "\n",
    "Based on this tutorial: https://www.jetson-ai-lab.com/tutorial_nano-llm.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe241a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miroslaw/.local/lib/python3.8/site-packages/transformers/modeling_attn_mask_utils.py:88: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_shape[-1] > 1 or self.sliding_window is not None:\n",
      "/home/miroslaw/.local/lib/python3.8/site-packages/transformers/modeling_attn_mask_utils.py:164: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if past_key_values_length > 0:\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"distilgpt2\"  # Use the same model as the chat code\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Dummy input (required for ONNX export)\n",
    "dummy_input = tokenizer(\"Hello world!\", return_tensors=\"pt\")\n",
    "\n",
    "# Export to ONNX format\n",
    "torch.onnx.export(\n",
    "    model, \n",
    "    (dummy_input[\"input_ids\"],),\n",
    "    \"distilgpt2.onnx\",  # Match the ONNX file name to the model\n",
    "    input_names=[\"input_ids\"],\n",
    "    output_names=[\"logits\"],\n",
    "    dynamic_axes={\n",
    "        \"input_ids\": {1: \"sequence_length\"},\n",
    "        \"logits\": {1: \"sequence_length\"}\n",
    "    },\n",
    "    opset_version=14\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6875ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next token predicted: 's\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "# Load ONNX model\n",
    "session = ort.InferenceSession(\"distilgpt2.onnx\")\n",
    "\n",
    "# Prepare tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "# Encode input\n",
    "inputs = tokenizer(\"The weather is \", return_tensors=\"np\")\n",
    "\n",
    "# Run inference\n",
    "ort_inputs = {'input_ids': inputs[\"input_ids\"]}\n",
    "logits = session.run([\"logits\"], ort_inputs)[0]\n",
    "\n",
    "# Generate next token\n",
    "next_token_id = np.argmax(logits[0, -1, :])\n",
    "next_token = tokenizer.decode([next_token_id])\n",
    "\n",
    "print(\"Next token predicted:\", next_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1dedb5",
   "metadata": {},
   "source": [
    "## Now a chat model\n",
    "\n",
    "Please note that we need to change the model to be a bit bigger, because the previous one does not really provide a good chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f99bcb2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'optimum'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# --- Export to ONNX (update model and directory) ---\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptimum\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01monnxruntime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ORTModelForCausalLM\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      6\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistilgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'optimum'"
     ]
    }
   ],
   "source": [
    "# --- Export to ONNX (update model and directory) ---\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from optimum.onnxruntime import ORTModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "dummy_input = tokenizer(\"Hello world!\", return_tensors=\"pt\")\n",
    "torch.onnx.export(\n",
    "    model, \n",
    "    (dummy_input[\"input_ids\"],),\n",
    "    \"distilgpt2.onnx\",\n",
    "    input_names=[\"input_ids\"],\n",
    "    output_names=[\"logits\"],\n",
    "    dynamic_axes={\n",
    "        \"input_ids\": {1: \"sequence_length\"},\n",
    "        \"logits\": {1: \"sequence_length\"}\n",
    "    },\n",
    "    opset_version=14)\n",
    "\n",
    "model_id = \"distilgpt2\"\n",
    "save_directory = \"./distilgpt2-chat-onnx-simple\"\n",
    "\n",
    "ort_model = ORTModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    export=True,\n",
    "    use_cache=False,\n",
    "    use_io_binding=False)\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "ort_model.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ecd353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:Show example of code?\n",
      "Assistant:Show example of code?\n",
      "Assistant:Show example of code?\n",
      "Assistant:Show example of code?\n",
      "Assistant:Show example of code?\n",
      "Assistant:Show example of code?\n",
      "Assistant:Show example of code?\n",
      "Assistant:Show example\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "model_dir = \"./distilgpt2-chat-onnx-simple\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "session = ort.InferenceSession(f\"{model_dir}/model.onnx\")\n",
    "\n",
    "def generate_response(prompt, max_new_tokens=50):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"np\")\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        position_ids = np.arange(input_ids.shape[1]).reshape(1, -1).astype(np.int64)\n",
    "        ort_inputs = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"position_ids\": position_ids\n",
    "        }\n",
    "        logits = session.run(None, ort_inputs)[0]\n",
    "        next_token_logits = logits[:, -1, :]\n",
    "        next_token_id = np.argmax(next_token_logits, axis=-1).reshape(1, 1)\n",
    "        input_ids = np.concatenate([input_ids, next_token_id], axis=-1)\n",
    "        attention_mask = np.concatenate(\n",
    "            [attention_mask, np.ones((attention_mask.shape[0], 1), dtype=np.int64)], \n",
    "            axis=-1\n",
    "        )\n",
    "        if next_token_id[0, 0] == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    return tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "prompt = \"User:Who is Fibonacci?\\nAssistant:\"\n",
    "response = generate_response(prompt)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
